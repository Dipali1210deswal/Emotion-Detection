{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5043236a-31a9-4686-807a-23ea22d40ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 35887/35887 [06:15<00:00, 95.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Function to convert string to integer\n",
    "def atoi(s):\n",
    "    n = 0\n",
    "    for i in s:\n",
    "        n = n*10 + ord(i) - ord(\"0\")\n",
    "    return n\n",
    "\n",
    "# Create directories for dataset\n",
    "outer_names = ['test', 'train']\n",
    "inner_names = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral']\n",
    "os.makedirs('data', exist_ok=True)\n",
    "for outer_name in outer_names:\n",
    "    os.makedirs(os.path.join('data', outer_name), exist_ok=True)\n",
    "    for inner_name in inner_names:\n",
    "        os.makedirs(os.path.join('data', outer_name, inner_name), exist_ok=True)\n",
    "\n",
    "# Initialize counters for each emotion category\n",
    "emotion_counts = {\n",
    "    'angry': 0, 'disgusted': 0, 'fearful': 0, 'happy': 0, 'sad': 0, 'surprised': 0, 'neutral': 0,\n",
    "    'angry_test': 0, 'disgusted_test': 0, 'fearful_test': 0, 'happy_test': 0, 'sad_test': 0, 'surprised_test': 0, 'neutral_test': 0\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r'C:\\Users\\DeLL\\OneDrive\\Desktop\\dipali\\minor proj\\Emotion detection\\fer2013.csv')\n",
    "\n",
    "mat = np.zeros((48, 48), dtype=np.uint8)\n",
    "print(\"Saving images...\")\n",
    "\n",
    "# Process and save images\n",
    "for i in tqdm(range(len(df))):\n",
    "    txt = df['pixels'][i]\n",
    "    words = txt.split()\n",
    "    \n",
    "    # Convert pixel values to image matrix\n",
    "    for j in range(2304):\n",
    "        xind = j // 48\n",
    "        yind = j % 48\n",
    "        mat[xind][yind] = atoi(words[j])\n",
    "\n",
    "    img = Image.fromarray(mat)\n",
    "\n",
    "    # Save images to respective directories based on emotion category\n",
    "    if i < 28709:\n",
    "        emotion = df['emotion'][i]\n",
    "        emotion_name = inner_names[emotion]\n",
    "        img.save(f'data/train/{emotion_name}/im{emotion_counts[emotion_name]}.png')\n",
    "        emotion_counts[emotion_name] += 1\n",
    "    else:\n",
    "        emotion = df['emotion'][i]\n",
    "        emotion_name = inner_names[emotion] + '_test'\n",
    "        img.save(f'data/test/{emotion_name.replace(\"_test\", \"\")}/im{emotion_counts[emotion_name]}.png')\n",
    "        emotion_counts[emotion_name] += 1\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7cc8fb-a016-4b51-98ad-e9d5b7a7b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 142ms/step - accuracy: 0.2406 - loss: 1.8335 - val_accuracy: 0.2825 - val_loss: 1.7475\n",
      "Epoch 2/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 101ms/step - accuracy: 0.2805 - loss: 1.7461 - val_accuracy: 0.3674 - val_loss: 1.6234\n",
      "Epoch 3/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 101ms/step - accuracy: 0.3265 - loss: 1.6788 - val_accuracy: 0.4159 - val_loss: 1.5160\n",
      "Epoch 4/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 103ms/step - accuracy: 0.3661 - loss: 1.6126 - val_accuracy: 0.4501 - val_loss: 1.4307\n",
      "Epoch 5/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 106ms/step - accuracy: 0.4084 - loss: 1.5407 - val_accuracy: 0.4816 - val_loss: 1.3618\n",
      "Epoch 6/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 101ms/step - accuracy: 0.4270 - loss: 1.4890 - val_accuracy: 0.4936 - val_loss: 1.3318\n",
      "Epoch 7/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 102ms/step - accuracy: 0.4369 - loss: 1.4577 - val_accuracy: 0.5109 - val_loss: 1.3054\n",
      "Epoch 8/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 103ms/step - accuracy: 0.4477 - loss: 1.4422 - val_accuracy: 0.5170 - val_loss: 1.2731\n",
      "Epoch 9/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 113ms/step - accuracy: 0.4599 - loss: 1.4203 - val_accuracy: 0.5279 - val_loss: 1.2320\n",
      "Epoch 10/10\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 150ms/step - accuracy: 0.4693 - loss: 1.3925 - val_accuracy: 0.5245 - val_loss: 1.2358\n",
      "Model saved successfully at: C:/Users/DeLL/OneDrive/Desktop/dipali/minor proj/Emotion detection/emotion_model.keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "import os\n",
    "\n",
    "# Data augmentation\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load data\n",
    "train_data = datagen_train.flow_from_directory('data/train', target_size=(48, 48), color_mode='grayscale', batch_size=64, class_mode='categorical')\n",
    "val_data = datagen_val.flow_from_directory('data/test', target_size=(48, 48), color_mode='grayscale', batch_size=64, class_mode='categorical')\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Input(shape=(48, 48, 1)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data, validation_data=val_data, epochs=10)\n",
    "\n",
    "# Specify full path for saving the model in native Keras format (.keras)\n",
    "model_save_path = 'C:/Users/DeLL/OneDrive/Desktop/dipali/minor proj/Emotion detection/emotion_model.keras'\n",
    "\n",
    "# Save the model\n",
    "save_model(model, model_save_path)\n",
    "\n",
    "print(\"Model saved successfully at:\", model_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb2e1bd-51a5-437d-b9da-47db22d48ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load pre-trained face detection model\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_faces(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.3, minNeighbors=5)\n",
    "    return faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bf50809-d841-493c-8b9d-abf34cedf372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, target_size=(48, 48)):\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image = image.astype('float32') / 255\n",
    "    image = np.expand_dims(image, axis=-1)\n",
    "    return image\n",
    "\n",
    "def predict_emotion(face_image, model):\n",
    "    preprocessed_image = preprocess_image(face_image)\n",
    "    predictions = model.predict(np.expand_dims(preprocessed_image, axis=0))\n",
    "    emotion = np.argmax(predictions)\n",
    "    return emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "709aebb6-446c-48b0-930f-6f4041e104e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load pre-trained face detection model\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('emotion_model.h5')\n",
    "\n",
    "# Compile the model (suppress warning)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define emotion categories\n",
    "emotion_labels = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral']\n",
    "\n",
    "def detect_faces(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.3, minNeighbors=5)\n",
    "    return faces\n",
    "\n",
    "def preprocess_image(image, target_size=(48, 48)):\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image = image.astype('float32') / 255\n",
    "    image = np.expand_dims(image, axis=-1)\n",
    "    return image\n",
    "\n",
    "def predict_emotion(face_image, model):\n",
    "    preprocessed_image = preprocess_image(face_image)\n",
    "    predictions = model.predict(np.expand_dims(preprocessed_image, axis=0))\n",
    "    emotion = np.argmax(predictions)\n",
    "    return emotion\n",
    "\n",
    "st.title('Real-Time Emotion Detection')\n",
    "\n",
    "run = st.checkbox('Run')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while run:\n",
    "    _, frame = video_capture.read()\n",
    "    faces = detect_faces(frame)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        emotion = predict_emotion(face, model)\n",
    "        \n",
    "        # Map emotion index to emotion label\n",
    "        emotion_label = emotion_labels[emotion]\n",
    "        \n",
    "        # Display the results\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, emotion_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    \n",
    "    st.image(frame, channels=\"BGR\")\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e9fc0-326a-4744-98ab-48fd6e04de07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
